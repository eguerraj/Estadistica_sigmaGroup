---
title: "Lab 1 Assignment"
author: "Xavi"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(tidyr)
library(stringr)
library(kableExtra)
library(janitor) # examining and cleaning dirty data
# For pretty doc:
# install.packages("prettydoc")
```


# CRISP-DM
This is a well known framework, which is normally used and implemented in any Data Science, Data Mining or Machine Learning Project. 

CRISP-DM is a cross-industry process for data mining. The methodology describes a proven approach to planning a data  project. 


The steps of this Framework are:

```{r, echo=FALSE}
str1 <- c("Business Understanding","Data Understanding","Data Preparation","Modeling","Evaluation","Deployment")
str2 <- c("Focuses on understanding the Business","Gathering data, Describing data, Exploring data and Verifying data quality","Prepare and clean the provided data, Outlier Treatment and Feature Engineering","Implementation of different Models algorithms","Accuracy, Sensitivity, Specificity, F-Score, AUC, R-Sq, Adj R-SQ, RMSE","A/B Testing and PROD/Live data")
x <- 1:6
dfaux <- data.frame(Step=x,Name=str1,Description=str2)
dfaux %>% kable() %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

For further details check:
* <https://www.sv-europe.com/wp-content/uploads/2011/07/CRISP_Analytical_Methodology.pdf>
* <https://inseaddataanalytics.github.io/INSEADAnalytics/CRISP_DM.pdf>
* <https://www.the-modeling-agency.com/crisp-dm.pdf>

Let's start describing our project related with EEG data. 

# EEG Data Analysis Project
We want to find out if two group of people that we labelled as alcoholic and a control group (or non-alcoholic) response in the same way in front of visual stimulus.
In other words, can a signal (or a group of electrical signals) coming from an EEG headset identify if a person is alcoholic or non-alcoholic without a blood analysis? 
Description of the experiment: it was a large study to examine EEG correlates of genetic predisposition to alcoholism. It contains measurements from 64 electrodes placed on subject's scalps which were sampled at 256 Hz (3.9-msec epoch) for 1 second. 

There were two groups of subjects: alcoholic and control. Each subject was exposed to either a single stimulus (S1) or to two stimuli (S1 and S2) which were pictures of objects chosen from the 1980 Snodgrass and Vanderwart picture set. When two stimuli were shown, they were presented in either a matched condition where S1 was identical to S2 or in a non-matched condition where S1 differed from S2. 
  

> The location of the 64 sensors looks similar to this image

<center>
![eeg Locations](images/eegElectrodePos.png)

![Brain Motor Sensory ](images/BrainMotorSensory.png)




![Functional Areas Cerebrum ](images/functionaAreasCerebrum.jpg)


</center>


The goal is to work alongside statistic lectures to enhance our ability to tackle real data problems. Throuhout these sessions each student should be able to:

* Perform Exploratory Data Analysis
    + Compute statistics (central tendency and variability)
    + Represent data insights using visual representations (histograms, boxplot, Scatter plots, etc using _ggplot2_ or _plotly_ library)
    + Use base R commands or _dplyr_ commands to solve problems with data
* Use simulations to solve probability problems
    + Generate join probability tables if two events are involved
    + Solve conditional or marginal or join probabilities
* Understand concepts involving random variables (either discrete or continuous)
    + Binomial
    + Hypergeometric
    + Poisson
    + Gaussian
* Apply Inference using
    + Confidence Intervals
    + Hypothesis Testing


# Problem Understanding
We need to understand the problem: Alcoholism and its effects on the brain, and what is EEG?  Briefly we can say the electroencephalogram (EEG) is a recording of the electrical activity of the brain from the scalp using sensors.

Let us first describe why it is used and the main areas of applications:
+ Neuromarketing
+ Human Factor (psychology)
+ Social Interaction
+ Neuroscience
+ Clinical and psychiatric studies
+ BCI

Take a look at some applications and uses in:

<https://www.hopkinsmedicine.org/healthlibrary/test_procedures/neurological/electroencephalogram_eeg_92,P07655>

A more detailed version of the EEG: quoted from (<https://www.ncbi.nlm.nih.gov/books/NBK390346/>)

> “The EEG is thought to be primarily generated by cortical pyramidal neurons in the cerebral cortex that are oriented perpendicularly to the brain's surface. The neural activity detectable by the EEG is the summation of the excitatory and inhibitory postsynaptic potentials of relatively large groups of neurons firing synchronously. Conventional scalp or cortical surface–recorded EEG is unable to register the momentary local field potential changes arising from neuronal action potentials”

Here are described some of the requirements of this step, all involving Business understanding:

* Identify the goal and frame the business problem.
* Gather information on resource, constraints, assumptions, risks etc.
* Prepare Analytical Goal.
* Flow Chart.


## Identify the goal and frame the business problem
We need to determine the business objectives. From a business perspective, what the customer really wants to accomplish. The goal can be uncovered by key factors. 

> This step tries to avoid a great deal of effort producing the right answers to the wrong questions

As an example, a primary business goal might be:

* How does the control group response to each experiment type?
* How does the alcohol group response to each channel?
* What metric can be build to asses whether a user is alcoholic or nonalcoholic?


##  Gather information on resource, constraints, assumptions, risks etc
Here it is supposed to study factors that should be considered in determining the data analysis goal and project plan.

Also it is important to detail a list of the assumptions made by the project and a list of all requirements of the project. If there are any constraints, they should be stated and described (such as size of the data set, software selected, etc.)

## Prepare Analytical Goal
This section involves stating project objectives in technical terms. 

As a clarification example:


* The business goal might be: _Increase user knowledge behavior_
* The Analytical Goal: _Predict how many users exceed a certain threshold over each channel, experiment type, or other measures, etc._

Define the criteria for a successful outcome to the project in technical terms (i.e. a certain level of predictive accuracy)


## Flow Chart
Produce a project plan. There it should specify the steps to be performed during the rest of the project. If the project is real then, the project should describe the stages to be executed in the project, together with their duration, resources required, inputs, outputs, and dependencies.

There is a clear element in this step: _the deadline_


# Data Understanding

The data come from:

<https://archive.ics.uci.edu/ml/datasets/eeg+database>

It contains measurements from 64 electrodes placed on the scalp sampled at 256 Hz

> Data Set Information:

This data arises from a large study to examine EEG correlates of genetic predisposition to alcoholism. It contains measurements from 64 electrodes placed on subject's scalps which were sampled at 256 Hz (3.9-msec epoch) for 1 second. 

Sampled at 256 Hz $\Rightarrow$ 256 samples per second

> Attribute Information:

Each trial is stored in its own file and will appear in the following format. 

```{r}
# co2a0000364.rd 
# 120 trials, 64 chans, 416 samples 368 post_stim samples 
# 3.906000 msecs uV 
# S1 obj , trial 0 
# FP1 chan 0 
print("0 FP1 0 -8.921 ")
print("0 FP1 1 -8.433 ")
print("0 FP1 2 -2.574 ")
print("0 FP1 3 5.239 ")
print("0 FP1 4 11.587 ")
print("0 FP1 5 14.028 ")
print("... ")
```

The first four lines are header information. Line 1 contains the subject identifier and indicates if the subject was an alcoholic (a) or control (c) subject by the fourth letter. Line 4 identifies the matching conditions: a single object shown (S1 obj), object 2 shown in a matching condition (S2 match), and object 2 shown in a non matching condition (S2 nomatch). 

Line 5 identifies the start of the data from sensor FP1. The four columns of data are: the trial number, sensor position, sample number (0-255), and sensor value (in micro volts). 

For further information take a look at the URL:

<https://archive.ics.uci.edu/ml/datasets/eeg+database>


Once the plan has been made, we move to the next step. In order to succeed, the plan depends on having the right data. In this step you must describe where the data was acquiered, or from which site was available. If there is any restriction in its use you should note it.

It is necessary to describe the data requirements (coming from the first step) and chek if this data is available from our sources (data base, URL, files, etc)

## Collect initial data
We need to acquire the data. Whatever the sources are it is important to specify them(if there are any integration should be an additional issue).

It is advasable, to generate an initial data collection report, where the method used to read the data is provided. A list of problems found and any resolutions achieved can be described as well in the report.

## Describe the data
Describe the data that has been acquired, including the format of the data and the dimensionality of data.

```{r}
#load Data
fileName <- "trainEEGdata/fullEEGdataSet.RDS"
eegdf <- readRDS(fileName)
###
names <- c("userId","expType","rep","chanNumb","chanName","timeInst","mVolts","userType")
dataTypes <- c("int","chr","int","int","chr","int","num","chr")
dataUnits <- c("User Identification","Experiment Type","Number of trial/ repetition","Channel number","Channel Name","Time instance","Micro Volts","User type Alch/non-ALch")
dfstr1 <- data.frame(Name=names,DataType=dataTypes,DataUnits=dataUnits)
dfstr1 %>%
  kable() %>%
  kable_styling()
```

See additional information:

* <https://neuroimage.usc.edu/brainstorm/Tutorials>

## Explore the Data
This task addresses questions using querying, visualization, and reporting techniques. Among them:

* Central Tendency, Variation and other measures
    + Mean, median, mode, max, min, harmonic mean, etc.
    + Standar deviation, IQR, range, 
    + Coefficient of Variation, Kurtosis, correlation, etc.
* Visualization
    + Histograms, box-plot, scatter-plot
    + 
    
See additional information:

* <https://towardsdatascience.com/intro-to-descriptive-statistics-252e9c464ac9>
* <https://csgillespie.github.io/efficientR/data-carpentry.html>
* <https://garthtarr.github.io/meatR/janitor.html>
* <https://blog.revolutionanalytics.com/2019/02/bbc-r-cookbook.html>
* <https://beta.rstudioconnect.com/content/3350/dplyr_tutorial.html>

Here we can show some insights and relation between variables:

```{r}
require(GGally)


# Example
auxdf <- eegdf %>% filter(userId==364,expType=="S1 obj") %>% select(chanNumb,timeInst,mVolts)
ggpairs(auxdf)
```


> The distribution of the variables?




And with some categorical variable a problem is shown clearly

```{r,warning=FALSE,message=FALSE}
library(viridis)
library(ggridges)
auxdf <- eegdf %>% filter(userId==364,expType=="S1 obj") %>% select(chanName,timeInst,mVolts)
ggplot(auxdf, aes(x = mVolts, y = as.factor(chanName), fill = ..x..)) +
  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01)+
  scale_fill_viridis(name = "micro Volts. [V]", option = "C") +
  labs(title = 'EEG micro Volts')
```

## Verify Data Quality
Examine the quality of the data. Figure out how to report on whether the data is complete or incomplete, whether there is outliers or none, whether the is missing values or none.

 If the verification brings some errors, how common are they?  if errors concern missing values in the data, how are they represented?
 
 You could generate a data quality report. 

See additional information:

* <https://cran.r-project.org/web/packages/dlookr/vignettes/diagonosis.html>
* <https://cran.r-project.org/web/packages/ggridges/vignettes/introduction.html>



# Data Preparation
How to load the data? 
Different options available: 

+ Web crawling
+ Download files

In each case there are advantages and disadvantages. For instance, using web crawling you can automatize the process completely. However it can handle some troubles depending on the approach. 

Another important concern is how to save the data once read it?
In which format the data needs to be keep it? There is some paper that deals with that issue in a convenient way (see Tidy Data Hadley Wickham): 

<https://www.jstatsoft.org/article/view/v059i10>

A possible solution is to keep the data in a matrix format where first column might be the user identifier, then if user is alcoholic (0 = non-alcoholic), then the paradigm (0 = S1 obj, 1 = S2 match, 2 = S2 nomatch), then repetition number and the channel number (from 1 to 64). From this column it is enough to keep the remaining columns as the 256 readings going from the first one to the last.

Another option is to read it in the given format


## Load Data Set

```{r, eval=FALSE}
# load the data file
fileName <- "trainEEGdata/fullEEGdataSet.RDS"
eegdf <- readRDS(fileName)
```

## Check the Data Object

```{r}
# length of the data set
dim(eegdf)
# Structure of the object
str(eegdf)
```

## Numerical Summaries

### What can you explain about these Variables?

```{r randomSelection, echo=FALSE}
set.seed(137) # seed for the random number generator
myVar <- names(eegdf)
howManyVar <- 3
workingVar <- sample(myVar,howManyVar,replace = FALSE)
```

> `r workingVar`


### What can you explain about these Variables?

```{r randomSelection2, echo=FALSE}
setdiff(myVar, workingVar)
```



### Central Tendency and Variation


* Statistics
+ mean = 
+ median = 
+ Standard Deviation = 
  
  \[
    s = \sqrt{\frac{\sum(x_i-\overline{x})^2}{n-1}}
  \]


## Graphical Representation


> Histogram

> Box-Plot

> Scatter Plot


## What you need to Solve

> P1) Take a random user and compute from this user these statistics from **mVolts**

+ min = min,
+ infLim = Q1 - 1.5 IQR
+ q05 = quantile(., 0.05),
+ q25 = quantile(., 0.25),
+ median = median,
+ q75 = quantile(., 0.75),
+ q95 = quantile(., 0.95),
+ upLim = Q3 + 1.5 IQR
+ max = max,
+ mean = mean,
+ sd = sd,
+ range = max - min,
+ IQR = Q3 - Q1


> P1.A) Compute the same according to the following condition:

+ expType
+ chanName (only those with AF_)


> P1.B) Compute the same according to both conditions

+ expType and chanName (only those with AF_)
+ expType and chanName (only those with PO_)


> P1.C) Compute the correlation between each signal (through timeInst) for each expType

+ chanName (only those with AF_)
+ chanName (only those with PO_)

**Hint** Talk with your instructor

> P2) Histogram and Boxplot of the variable **mVolts**

Do it allways for:

+ chanName (only those with AF_)
+ chanName (only those with PO_)

> P2.A) Histogram for each channel (Fix the expType)

+ geom_histogram
+ geom_density
+ geom_vline

> P2.B) BoxPlot for each channel (Fix the expType)

+ Boxplot
+ Violin (with mean and median)


> P3) Take the user and compute the average Signal (Fix the expType) and represent the signal

Do it allways for:

+ chanName (only those with AF_)
+ chanName (only those with PO_)



# Probability

The formal definition is:
  
  \[
    P(X = x, Y = y) = P (X = x \cap Y = y)
  \]

The whole point of the joint distribution is to look for a relationship between two variables. For example, the following table shows some probabilities for _X=ChanName_ and _Y=catVolts_ happening at the same time: You can use the table to find probabilities.


Generate a Joint probability table with the following variables:


## To convert from numeric to categorical


> Use **cut** and breaks at 0.99, .95, .75, .50, .25, .05, .01




```{r}
auxdf <- eegdf %>% filter(userId==364)
#str(auxdf)
quantVal <- c(0.01,0.05,0.25,0.50,0.75,0.95,0.99)
breaksQVal <- quantile(auxdf$mVolts,quantVal)
breaksQVal <- c(-Inf,breaksQVal,+Inf)
breakLab <- as.character(paste("V",quantVal,sep=""))
breakLab <- c(breakLab,"V1.00")
auxdf <- auxdf %>% mutate(catVolts=cut(mVolts, breaks=breaksQVal, labels=breakLab))
# str(auxdf) We add the categorical Variable catVolts
```


- _catVolts_
- _chanName_ chanName (only those with AF_)



## Table Format


> Getting the number of cases

```{r}
resP21 <- auxdf %>% filter(str_detect(chanName,"^AF")) %>% 
  group_by(catVolts, chanName) %>%
  summarise (n = n())

# Easy output
resP21 %>% spread(key = catVolts, value = n)

# Formated Output
resP21 %>%  spread(key = catVolts, value = n) %>% kable() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% 
  column_spec(1, bold = T, border_right = T, background = "yellow") 
```


> Use Probability


```{r}
auxRes21 <- resP21
totElem <- sum(auxRes21$n) # Account for the total number of elements

tabRes21 <- auxRes21$n/totElem
auxRes21$n <- tabRes21
auxRes21 %>% spread(key = catVolts, value = n) %>% kable() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% 
  column_spec(1, bold = T, border_right = T, background = "yellow") 
```


### Option: use Janitor library


```{r}
resP22 <- auxdf %>% filter(str_detect(chanName,"^AF")) %>%
tabyl(chanName,catVolts) 

resP22 %>%  kable() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% 
  column_spec(1, bold = T, border_right = T, background = "yellow")
```


```{r}
resP22[,2:9] <- resP22[,2:9]/totElem

resP22 %>%  kable() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% 
  column_spec(1, bold = T, border_right = T, background = "yellow")
```


## What you need to Solve


> P1) All marginal probabilities

+ Represent the distribution of the marginal variables (table and plot)

> P2) Compute at least 6 conditional Probabilities with variables given by 

+ _X=ChanName_ (3 cases)
+ _Y=catVolts_ (3 cases)

> P3) Compute at least 2 probabilities that involve Intersection

> P4) Compute at least 2 probabilities that involve Union

> P5) Compute at least 2 probabilities that involve Complement




> P6*) Optional can you generate a 3D of the Joint Probability Table



# Random Variable Problems


## Generate the following Binary Variables

Creation of the _highImpulse_ to be 1 if value is higher than 10 and 0 otherwise. And _insideIQR_ to be 1 if the magnitute is between Q1 and Q3, and 0 otherwise

```{r}
# the easiest way to create a Factor variable
auxdf <- auxdf %>% mutate(highImpulse = ifelse(mVolts<10,0,1))

quart <- quantile((auxdf %>% select(mVolts))$mVolts, probs = c(0.25,0.75))
auxdf <- auxdf %>% mutate(insideIQR = ifelse(mVolts<quart[1]|mVolts>quart[2],0,1))
str(auxdf)
```




## Generate a Joint Probability Table

```{r}
auxdf %>% 
  group_by(highImpulse,insideIQR) %>% 
  summarise(n= n())


auxdf %>% 
  group_by(highImpulse,insideIQR) %>% 
  summarise(n= n()) 

#with the library Janitor
auxdf %>% tabyl(highImpulse,insideIQR)

myTab <- auxdf %>% tabyl(highImpulse,insideIQR)
myTab <- myTab[,2:3]
mytab.per <- myTab %>% prop.table() 

# To represent has a table
mytab.per
```


## Discrete Random Variables

### A.- Binomial
A Binomial distribution is defined as a number of successes in a sequence of independent Bernoulli trials. 

\[
X = Bin(n,p) \\
n = \textrm{number of trials} \\
p = \textrm{probability of success} \\
P(x) = \binom{n}{x} p^x (1-p)^{n-x} \\
E[X] = np \\
V[X] = np(1-p) \\
Desv[X] = \sqrt{np(1-p)} \\
\]

* Generate a problem using the variable *insideIQR* that involves working with a Binomial random variable.
    + Define the random variable *X*
    + Compute the PMF (show as table and as plot)
    + Solve at least two problems involving probability 
    + Solve at least one problem involving $F^{-1}$ (inverse probability problem)
    + Using _set.seed(123)_ generate 4 simulations with 10, 100, 1000, and 10000 and show results of the simulated PMF using table and plot. It is a plus to compare with theoretical values



### B.- HyperGeometric Distribution
An Hypergeometric Distribution is defined as the number of success _(without replacement)_ in our sample of size **n** 


\[
X = HyperGeom(N,k,n) \\
N = \textrm{Total number of elements} \\
k = \textrm{successful elements} \\
n = \textrm{sample size} \\
  \\
P(x) = \frac{ \binom{k}{x} \binom{N-k}{n-x} } {\binom{N}{n}} \\
  \\
E[X] = \frac{nk}{N} \\
V[X] = \frac{k(N-k)n(N-n)}{N^2(N-1)} \\
Desv[X] = \sqrt{\frac{k(N-k)n(N-n)}{N^2(N-1)}} \\
\]

* Generate a problem using the variable *insideIQR* that involves working with a Hypergeometric random variable.
    + Define the random variable *X*
    + Compute the PMF (show as table and as plot)
    + Solve at least two problems involving probability 
    + Solve at least one problem involving $F^{-1}$ (inverse probability problem)
    + Using _set.seed(123)_ generate 4 simulations with 10, 100, 1000, and 10000 and show results of the simulated PMF using table and plot. It is a plus to compare with theoretical values



### C.- Poisson Distribution
A Poisson Distribution is defined as the number of success within a fixed period of time 


\[
X = Pois(\lambda) \\
\lambda = \textrm{frequency, average number of events} \\
k = \textrm{successful elements} \\
  \\
P(x) = e^{-\lambda}\frac{\lambda^x}{x!} \\
  \\
E[X] = \lambda \\
V[X] = \lambda \\
Desv[X] = \sqrt{\lambda} \\
\]

* Generate a problem using the variable *insideIQR* that involves working with a Poisson random variable.
    + Define the random variable *X*
    + Compute the PMF (show as table and as plot)
    + Solve at least two problems involving probability 
    + Solve at least one problem involving $F^{-1}$ (inverse probability problem)
    + Using _set.seed(123)_ generate 4 simulations with 10, 100, 1000, and 10000 and show results of the simulated PMF using table and plot. It is a plus to compare with theoretical values


## Continuous Random Variable

### D.- Normal Distribution
The gaussian (or bell-shaped) distribution is the most important continuous distribution. Why? Normality arises naturally in many real contexts ranging from physical to biological, from engineering to social measurement situations. The central limit theorem (CLT) states that under certain (fairly common) conditions, the sum of many random variables will have an approximately normal distribution. Normality is also important in statistical inference.


The normal distribution is described by two parameters:  the mean $\mu$ and the standard deviation $\sigma$ and the Normal Model is written : $X \sim Norm(\mu, \sigma)$


\[
X \sim Norm(\mu,\sigma) \\
\mu = \textrm{ mean or expectation of the distribution} \\
\sigma = \textrm{ standard deviation} \\ 
f_X(x) = \frac{1}{\sigma \sqrt {2\pi }}exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right) \\
E[X] = \mu \\
V[X] = \sigma^2 \\
Desv[X] = \sigma \\
\]


* Generate a problem using the variable *mVolts* that involves working with a Normal random variable.
    + Define the random variable *X*
    + Represent the PDF (show as plot)
    + Represent the Histogram of the *mVolts* and overlap the PDF
    + Solve at least two problems involving probability 
    + Solve at least one problem involving $F^{-1}$ (inverse probability problem)
    + Using _set.seed(123)_ generate 4 simulations with 10, 100, 1000, and 10000 and show results of the simulated CDF using table and plot. It is a plus to compare with theoretical values



### Percentile Computation
Find out where the percentiles of this normal distribution are. Each of the coloured bars represent the same amount of percentage.


```{r, echo=FALSE}
set.seed(1235)
mu <- runif(1,0,10000)
sigma <- rnorm(1,2000,500)
lenseq <- 2500 
quantiles <- sample(5:20,1)
z.df     <- data.frame(x = seq(from=mu-3.5*sigma, to=mu+3.5*sigma, length.out=lenseq))
z.df$pdf <- dnorm(z.df$x,mu,sigma)
z.df$qt  <- cut(pnorm(z.df$x,mu,sigma),breaks=quantiles,labels=F)

ggplot(z.df,aes(x=x,y=pdf))+
  geom_area(aes(x=x,y=pdf,group=qt,fill=qt),color="black")+
  scale_fill_gradient2(midpoint=median(unique(z.df$qt)), guide="none") +
  scale_x_continuous(breaks = round(seq(mu-4*sigma, mu+4*sigma, by = sigma),1)) +
  theme_bw()
```

Note that the mean of the distribution is $\mu=$ `r format(mu,digits=2)` and sigma is equal to $\sigma=$ `r format(sigma,digits=2)`.

* Generate a table with the percentiles computed





# Deadline
You should deliver your *.Rmd* file and the report generated (only *.PDF*) before

> 29th February 2020


Check you add the names at the _YAML_ header


> Do not forget Names of the Group


> Write in *authors* the names of the whole team members



# Feel Free to ask for ASSISTANCE


> Instructor will be happy to help